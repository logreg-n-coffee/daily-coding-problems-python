{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 354\n",
    "\n",
    "## Description\n",
    "\n",
    "Design a system to crawl and copy all of Wikipedia using a distributed network of machines.\n",
    "\n",
    "More specifically, suppose your server has access to a set of client machines. Your client machines can execute code you have written to access Wikipedia pages, download and parse their data, and write the results to a database.\n",
    "\n",
    "Some questions you may want to consider as part of your solution are:\n",
    "\n",
    "* How will you reach as many pages as possible?\n",
    "* How can you keep track of pages that have already been visited?\n",
    "* How will you deal with your client machines being blacklisted?\n",
    "* How can you update your database when Wikipedia pages are added or updated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing a system to crawl and copy all of Wikipedia using a distributed network of machines involves various challenges, especially with respect to rate limits, redundancy, and updates. Here's a proposed design for this system:\n",
    "\n",
    "### 1. Preliminaries:\n",
    "\n",
    "**Database**: Create a centralized database that has tables for:\n",
    "\n",
    "- **Visited URLs**: To keep track of already visited Wikipedia pages.\n",
    "- **Queue**: To keep track of the URLs to be visited.\n",
    "- **Data**: To store the content of the Wikipedia pages.\n",
    "\n",
    "**Seed URLs**: Begin with a list of seed URLs. These could be main pages, or lists of topics, or any starting points from where links can be discovered.\n",
    "\n",
    "### 2. Distributing Tasks to Clients:\n",
    "\n",
    "Each client machine will:\n",
    "\n",
    "1. Retrieve a batch of URLs from the **Queue** in the database.\n",
    "2. For each URL:\n",
    "   - Fetch the page content.\n",
    "   - Parse out links to other Wikipedia pages.\n",
    "   - Save the content to the **Data** table.\n",
    "   - Add the newly found URLs to the **Queue** (if not already visited).\n",
    "   - Mark the URL as visited in the **Visited URLs** table.\n",
    "\n",
    "### 3. Handling Blacklists:\n",
    "\n",
    "Since Wikipedia (and most sites) would not appreciate and might block crawlers that aggressively scrape content, consider the following:\n",
    "\n",
    "- **Rate Limiting**: Ensure each client respects a certain delay between requests. \n",
    "- **User-Agent Rotation**: Rotate through different User-Agent strings to mimic different types of browsers and devices.\n",
    "- **IP Rotation**: Use a pool of proxy servers to rotate IP addresses.\n",
    "- **Monitoring**: Have a system to detect if any client has been blacklisted (e.g., consistent failed requests). If detected, that client should pause requests and possibly switch to a new IP.\n",
    "\n",
    "### 4. Updates and Handling Wikipedia Changes:\n",
    "\n",
    "1. **Incremental Updates**: Maintain a timestamp in the **Visited URLs** table. Periodically re-visit pages after a certain time interval, and check for changes to update the database accordingly.\n",
    "   \n",
    "2. **Listen to Wikipedia's API**: Wikipedia has an API endpoint that streams recent changes. Listening to this can help in keeping the data updated in real-time.\n",
    "\n",
    "### 5. Scalability and Efficiency:\n",
    "\n",
    "1. **Batch Processing**: Instead of having each client retrieve one URL at a time, retrieve batches to reduce database access frequency.\n",
    "   \n",
    "2. **Distributed DB**: Consider using a distributed database system to handle large-scale data, like Apache Cassandra or Amazon DynamoDB.\n",
    "\n",
    "3. **Parallel Processing**: On each client machine, use parallel threads or processes to maximize efficiency.\n",
    "\n",
    "### 6. Data Integrity:\n",
    "\n",
    "1. **Checkpoints**: Periodically create checkpoints of the current state. If there's a failure, the system can revert to the last checkpoint.\n",
    "   \n",
    "2. **Validation**: Periodically cross-check a random set of pages against Wikipedia to ensure data hasn't become corrupted or outdated.\n",
    "\n",
    "### 7. Ethical Considerations:\n",
    "\n",
    "1. **Respect `robots.txt`**: Even though the goal is to copy all of Wikipedia, it's crucial to respect any rules laid out in Wikipedia's `robots.txt`. \n",
    "   \n",
    "2. **Declare Intent**: If scraping at a large scale, it might be a good practice to let Wikipedia know of the intent and purpose, so there's transparency in the process.\n",
    "\n",
    "3. **Data Usage**: Ensure that the copied data is used ethically, without violating Wikipedia's terms of use.\n",
    "\n",
    "### 8. Miscellaneous:\n",
    "\n",
    "1. **Handling Media**: Wikipedia has images, videos, and other media types. Decide how these will be stored and referenced.\n",
    "   \n",
    "2. **Error Handling**: Implement robust error handling. If a page fails to be fetched or parsed, log the error and possibly retry later.\n",
    "\n",
    "3. **Monitoring and Alerts**: Set up a monitoring system to track the progress of the crawl, any errors, blacklisted clients, etc., and receive alerts for any issues.\n",
    "\n",
    "This design provides a high-level overview of a system to crawl Wikipedia using a distributed network. Actual implementation would involve further details and optimizations, but these steps provide a starting point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
